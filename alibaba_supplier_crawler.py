import asyncio
import json
import sqlite3
import os
import time
import random
import re
from datetime import datetime
import aiohttp
import requests
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import urllib.parse
from urllib.parse import urlparse

class AlibabaSupplierCrawler:
    def __init__(self):
        self.db_path = "alibaba_supplier_data.db"
        self.init_database()
        
        # è¯»å–åˆ†ç±»æ•°æ®
        self.categories = self.load_categories()
    
    def log(self, message, level="INFO", log_callback=None):
        """æ—¥å¿—è®°å½•æ–¹æ³•"""
        if log_callback:
            log_callback(message, level)
        else:
            # æ ¹æ®æ—¥å¿—çº§åˆ«æ·»åŠ å‰ç¼€
            if level == "ERROR":
                prefix = "âŒ"
            elif level == "SUCCESS":
                prefix = "âœ…"
            elif level == "WARNING":
                prefix = "âš ï¸"
            elif level == "DEBUG":
                prefix = "ğŸ”"
            else:
                prefix = "â„¹ï¸"
            print(f"{prefix} {message}")
    
    async def save_suppliers_to_cache_file(self, suppliers, cache_file_base, log_callback=None):
        """å°†ä¾›åº”å•†æ•°æ®ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´åˆ†æ–‡ä»¶ï¼‰"""
        try:
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼ˆæ¯5åˆ†é’Ÿä¸€ä¸ªæ–‡ä»¶ï¼‰
            now = datetime.now()
            # è®¡ç®—5åˆ†é’Ÿé—´éš”çš„æ—¶é—´æˆ³
            interval_minutes = (now.hour * 60 + now.minute) // 5 * 5
            time_suffix = f"{now.strftime('%Y%m%d')}_{interval_minutes:04d}"
            
            # æ„å»ºå®é™…æ–‡ä»¶è·¯å¾„
            cache_dir = os.path.dirname(cache_file_base)
            base_name = os.path.splitext(os.path.basename(cache_file_base))[0]
            actual_cache_file = os.path.join(cache_dir, f"{base_name}_{time_suffix}.json")
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            if cache_dir and not os.path.exists(cache_dir):
                os.makedirs(cache_dir)
            
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = []
            if os.path.exists(actual_cache_file):
                try:
                    with open(actual_cache_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_data = []
            
            # æ·»åŠ æ–°æ•°æ®
            existing_data.extend(suppliers)
            
            # å†™å…¥æ–‡ä»¶
            with open(actual_cache_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
            if log_callback:
                log_callback(f"âœ… å·²å°† {len(suppliers)} ä¸ªä¾›åº”å•†æ•°æ®ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶: {actual_cache_file}")
            
            return actual_cache_file
            
        except Exception as e:
            error_msg = f"âŒ ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}"
            if log_callback:
                log_callback(error_msg, "ERROR")
            else:
                print(error_msg)
            return None
    
    async def batch_save_from_cache_file(self, cache_file, skip_duplicates=True, log_callback=None):
        """ä»ç¼“å­˜æ–‡ä»¶æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            if not os.path.exists(cache_file):
                if log_callback:
                    log_callback(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}", "WARNING")
                return 0
            
            # è¯»å–ç¼“å­˜æ•°æ®
            with open(cache_file, 'r', encoding='utf-8') as f:
                suppliers_data = json.load(f)
            
            if not suppliers_data:
                if log_callback:
                    log_callback("ğŸ“­ ç¼“å­˜æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ•°æ®éœ€è¦å…¥åº“")
                return 0
            
            if log_callback:
                log_callback(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å…¥åº“ï¼Œå…± {len(suppliers_data)} ä¸ªä¾›åº”å•†æ•°æ®")
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„company_idåˆ°å†…å­˜
            existing_company_ids = set()
            if skip_duplicates:
                if log_callback:
                    log_callback("ğŸ” æ­£åœ¨åŠ è½½å·²å­˜åœ¨çš„ä¾›åº”å•†IDåˆ°å†…å­˜...")
                
                conn = sqlite3.connect(self.db_path, timeout=10.0)
                cursor = conn.cursor()
                cursor.execute('SELECT company_id FROM suppliers')
                for row in cursor.fetchall():
                    existing_company_ids.add(row[0])
                conn.close()
                
                if log_callback:
                    log_callback(f"âœ… å·²åŠ è½½ {len(existing_company_ids)} ä¸ªå·²å­˜åœ¨çš„ä¾›åº”å•†ID")
            
            # è¿‡æ»¤å‡ºéœ€è¦ä¿å­˜çš„æ–°ä¾›åº”å•†
            new_suppliers = []
            skipped_count = 0
            
            for supplier in suppliers_data:
                if skip_duplicates and supplier['company_id'] in existing_company_ids:
                    skipped_count += 1
                    if log_callback and skipped_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªé‡å¤çš„
                        log_callback(f"  âœ“ è·³è¿‡é‡å¤ä¾›åº”å•†: {supplier['company_name']} (ID: {supplier['company_id']})")
                else:
                    # ç”Ÿæˆä¿å­˜è·¯å¾„
                    save_path = self.generate_save_path(supplier)
                    supplier['save_path'] = save_path
                    new_suppliers.append(supplier)
                    # æ›´æ–°å†…å­˜ä¸­çš„IDé›†åˆï¼Œé¿å…åŒä¸€æ‰¹æ¬¡å†…çš„é‡å¤
                    existing_company_ids.add(supplier['company_id'])
            
            if log_callback:
                log_callback(f"ğŸ“Š è¿‡æ»¤å®Œæˆï¼šæ–°å¢ {len(new_suppliers)} ä¸ªï¼Œè·³è¿‡é‡å¤ {skipped_count} ä¸ª")
            
            # æ‰¹é‡æ’å…¥æ–°ä¾›åº”å•†
            saved_count = 0
            if new_suppliers:
                if log_callback:
                    log_callback(f"ğŸ’¾ å¼€å§‹æ‰¹é‡æ’å…¥ {len(new_suppliers)} ä¸ªæ–°ä¾›åº”å•†...")
                
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…é•¿æ—¶é—´é”å®šæ•°æ®åº“
                batch_size = 50  # æ¯æ‰¹50ä¸ªä¾›åº”å•†
                total_batches = (len(new_suppliers) + batch_size - 1) // batch_size
                
                batch_num = 0
                while batch_num < total_batches:
                    start_idx = batch_num * batch_size
                    end_idx = min(start_idx + batch_size, len(new_suppliers))
                    batch_suppliers = new_suppliers[start_idx:end_idx]
                    
                    if log_callback:
                        log_callback(f"ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}ï¼ŒåŒ…å« {len(batch_suppliers)} ä¸ªä¾›åº”å•†")
                    
                    # æ¯æ‰¹ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥å’Œäº‹åŠ¡
                    conn = None
                    retry_count = 0
                    max_retries = 3
                    batch_success = False
                    
                    while retry_count < max_retries and not batch_success:
                        try:
                            if retry_count > 0 and log_callback:
                                log_callback(f"ğŸ”„ æ‰¹æ¬¡ {batch_num + 1} ç¬¬ {retry_count + 1} æ¬¡å°è¯•")
                            
                            if log_callback:
                                log_callback(f"ğŸ”— æ­£åœ¨è¿æ¥æ•°æ®åº“...")
                            
                            conn = sqlite3.connect(self.db_path, timeout=60.0)  # å¢åŠ è¶…æ—¶æ—¶é—´
                            conn.execute('PRAGMA journal_mode=DELETE')  # æ”¹ä¸ºDELETEæ¨¡å¼é¿å…WALé”å®š
                            conn.execute('PRAGMA synchronous=OFF')  # å…³é—­åŒæ­¥ä»¥æé«˜æ€§èƒ½
                            conn.execute('PRAGMA busy_timeout=60000')  # 60ç§’è¶…æ—¶
                            conn.execute('PRAGMA cache_size=10000')  # å¢åŠ ç¼“å­˜
                            cursor = conn.cursor()
                            
                            if log_callback:
                                log_callback(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå¼€å§‹äº‹åŠ¡")
                            
                            # ä½¿ç”¨æ™®é€šäº‹åŠ¡è€Œä¸æ˜¯IMMEDIATEï¼Œå‡å°‘é”å®š
                            conn.execute('BEGIN')
                            
                            if log_callback:
                                log_callback(f"ğŸ“ å¼€å§‹æ’å…¥æ‰¹æ¬¡ {batch_num + 1} çš„ {len(batch_suppliers)} ä¸ªä¾›åº”å•†")
                            
                            batch_saved = 0
                            for i, supplier in enumerate(batch_suppliers):
                                try:
                                    cursor.execute('''
                                        INSERT INTO suppliers (company_id, company_name, action_url, country_code, 
                                                           city, gold_years, verified_supplier, is_factory, 
                                                           review_score, review_count, company_on_time_shipping,
                                                           factory_size_text, total_employees_text, transaction_count_6months,
                                                           transaction_gmv_6months_text, gold_supplier, trade_assurance, response_time,
                                                           category_id, category_name, save_path)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                    ''', (
                                        supplier['company_id'],
                                        supplier['company_name'],
                                        supplier['action_url'],
                                        supplier['country_code'],
                                        supplier['city'],
                                        supplier['gold_years'],
                                        supplier['verified_supplier'],
                                        supplier['is_factory'],
                                        supplier['review_score'],
                                        supplier['review_count'],
                                        supplier.get('company_on_time_shipping', ''),
                                        supplier.get('factory_size_text', ''),
                                        supplier.get('total_employees_text', ''),
                                        supplier.get('transaction_count_6months', ''),
                                        supplier.get('transaction_gmv_6months_text', ''),
                                        supplier.get('gold_supplier', False),
                                        supplier.get('trade_assurance', False),
                                        supplier.get('response_time', ''),
                                        supplier.get('category_id', ''),
                                        supplier.get('category_name', ''),
                                        supplier['save_path']
                                    ))
                                    batch_saved += 1
                                    saved_count += 1
                                    
                                    # æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                    if (i + 1) % 10 == 0 and log_callback:
                                        log_callback(f"  ğŸ“Š æ‰¹æ¬¡ {batch_num + 1} è¿›åº¦: {i + 1}/{len(batch_suppliers)} å·²æ’å…¥")
                                
                                except Exception as e:
                                    if log_callback:
                                        log_callback(f"âš ï¸ æ’å…¥ä¾›åº”å•†å¤±è´¥ {supplier.get('company_name', 'Unknown')}: {str(e)}", "WARNING")
                                    continue
                            
                            # æäº¤å½“å‰æ‰¹æ¬¡
                            conn.commit()
                            batch_success = True
                            
                            if log_callback:
                                log_callback(f"âœ… æ‰¹æ¬¡ {batch_num + 1}/{total_batches} æˆåŠŸå®Œæˆ: ä¿å­˜ {batch_saved} ä¸ªä¾›åº”å•†")
                            
                            # çŸ­æš‚ä¼‘æ¯ï¼Œè®©å…¶ä»–æ“ä½œæœ‰æœºä¼šè®¿é—®æ•°æ®åº“
                            await asyncio.sleep(0.1)
                            
                        except sqlite3.OperationalError as e:
                            if conn:
                                try:
                                    conn.rollback()
                                    if log_callback:
                                        log_callback(f"ğŸ”„ æ‰¹æ¬¡ {batch_num + 1} äº‹åŠ¡å·²å›æ»š")
                                except Exception as rollback_e:
                                    if log_callback:
                                        log_callback(f"âš ï¸ å›æ»šå¤±è´¥: {str(rollback_e)}", "WARNING")
                            
                            if "database is locked" in str(e):
                                retry_count += 1
                                if log_callback:
                                    log_callback(f"âš ï¸ æ•°æ®åº“é”å®šï¼Œæ‰¹æ¬¡ {batch_num + 1} ç¬¬ {retry_count} æ¬¡é‡è¯• (æœ€å¤š {max_retries} æ¬¡)", "WARNING")
                                if retry_count < max_retries:
                                    await asyncio.sleep(retry_count * 2.0)  # é€’å¢å»¶è¿Ÿ
                                else:
                                    if log_callback:
                                        log_callback(f"âŒ æ‰¹æ¬¡ {batch_num + 1} é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡", "ERROR")
                            else:
                                if log_callback:
                                    log_callback(f"âŒ æ‰¹æ¬¡ {batch_num + 1} æ•°æ®åº“æ“ä½œé”™è¯¯: {str(e)}", "ERROR")
                                break  # éé”å®šé”™è¯¯ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        except Exception as e:
                            if conn:
                                try:
                                    conn.rollback()
                                    if log_callback:
                                        log_callback(f"ğŸ”„ æ‰¹æ¬¡ {batch_num + 1} äº‹åŠ¡å·²å›æ»š")
                                except Exception as rollback_e:
                                    if log_callback:
                                        log_callback(f"âš ï¸ å›æ»šå¤±è´¥: {str(rollback_e)}", "WARNING")
                            
                            if log_callback:
                                log_callback(f"âŒ æ‰¹æ¬¡ {batch_num + 1} æœªçŸ¥é”™è¯¯: {str(e)}", "ERROR")
                            break  # å…¶ä»–é”™è¯¯ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        finally:
                            if conn:
                                try:
                                    conn.close()
                                except Exception as close_e:
                                    if log_callback:
                                        log_callback(f"âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {str(close_e)}", "WARNING")
                    
                    if not batch_success:
                        if log_callback:
                            log_callback(f"âŒ æ‰¹æ¬¡ {batch_num + 1} æœ€ç»ˆå¤±è´¥ï¼Œå·²è·³è¿‡è¯¥æ‰¹æ¬¡", "ERROR")
                    
                    batch_num += 1
            
            # å…¥åº“å®Œæˆåæ¸…ç©ºç¼“å­˜æ–‡ä»¶
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump([], f)
            
            if log_callback:
                log_callback(f"âœ… æ‰¹é‡å…¥åº“å®Œæˆï¼æ–°å¢: {saved_count} ä¸ªï¼Œè·³è¿‡é‡å¤: {skipped_count} ä¸ª", "SUCCESS")
            
            return saved_count
            
        except Exception as e:
            error_msg = f"âŒ æ‰¹é‡å…¥åº“å¤±è´¥: {str(e)}"
            if log_callback:
                log_callback(error_msg, "ERROR")
            else:
                print(error_msg)
            return 0
        
    def change_database_path(self, new_db_path):
        """æ›´æ”¹æ•°æ®åº“è·¯å¾„å¹¶é‡æ–°åˆå§‹åŒ–"""
        self.db_path = new_db_path
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºä¾›åº”å•†è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS suppliers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company_id TEXT,
                company_name TEXT,
                action_url TEXT,
                country_code TEXT,
                city TEXT,
                gold_years TEXT,
                verified_supplier BOOLEAN,
                is_factory BOOLEAN,
                review_score TEXT,
                review_count INTEGER,
                company_on_time_shipping TEXT,
                factory_size_text TEXT,
                total_employees_text TEXT,
                transaction_count_6months TEXT,
                transaction_gmv_6months_text TEXT,
                gold_supplier BOOLEAN,
                trade_assurance BOOLEAN,
                response_time TEXT,
                category_id TEXT,
                category_name TEXT,
                save_path TEXT,
                license_extracted BOOLEAN DEFAULT FALSE,
                is_used BOOLEAN DEFAULT FALSE,
                extraction_failed_count INTEGER DEFAULT 0,
                skip_extraction BOOLEAN DEFAULT FALSE,
                last_extraction_attempt TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºæ‰§ç…§å›¾ç‰‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS licenses (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                supplier_id INTEGER,
                license_name TEXT,
                license_url TEXT,
                file_id TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (supplier_id) REFERENCES suppliers (id)
            )
        ''')
        
        # åˆ›å»ºæ‰§ç…§ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS license_info (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                supplier_id INTEGER,
                registration_no TEXT,
                company_name TEXT,
                date_of_issue TEXT,
                date_of_expiry TEXT,
                registered_capital TEXT,
                country_territory TEXT,
                registered_address TEXT,
                year_established TEXT,
                legal_form TEXT,
                legal_representative TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (supplier_id) REFERENCES suppliers (id)
            )
        ''')
        
        # åˆ›å»ºå…¬å¸æ³¨å†Œä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS company_registration (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                profile_id VARCHAR(100) NOT NULL,
                supplier_id VARCHAR(100) NOT NULL,
                registration_number VARCHAR(50) NOT NULL,
                company_name VARCHAR(200) NOT NULL,
                registered_address TEXT,
                province VARCHAR(50),
                city VARCHAR(50),
                district VARCHAR(50),
                zip_code VARCHAR(10),
                legal_representative VARCHAR(100),
                issue_date DATE,
                expiration_date DATE,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(supplier_id, registration_number)
            )
        ''')
        
        # ä¸ºcompany_registrationè¡¨åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_supplier_id ON company_registration(supplier_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_registration_number ON company_registration(registration_number)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_company_name ON company_registration(company_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_province_city ON company_registration(province, city)')
        
        # åˆ›å»ºè§¦å‘å™¨ï¼Œè‡ªåŠ¨æ›´æ–°updated_atå­—æ®µ
        cursor.execute('''
            CREATE TRIGGER IF NOT EXISTS update_company_registration_timestamp 
                AFTER UPDATE ON company_registration
                FOR EACH ROW
            BEGIN
                UPDATE company_registration 
                SET updated_at = CURRENT_TIMESTAMP 
                WHERE id = NEW.id;
            END
        ''')
        
        # åˆ›å»ºä»£ç†é…ç½®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS proxies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                host TEXT NOT NULL,
                port INTEGER NOT NULL,
                username TEXT,
                password TEXT,
                is_active BOOLEAN DEFAULT FALSE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æ’å…¥é»˜è®¤ä»£ç†é…ç½®ï¼ˆå¦‚æœè¡¨ä¸ºç©ºï¼‰
        cursor.execute('SELECT COUNT(*) FROM proxies')
        if cursor.fetchone()[0] == 0:
            cursor.execute('''
                INSERT INTO proxies (name, host, port, username, password, is_active)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', ('é»˜è®¤ä»£ç†', '127.0.0.1', 7890, 't15395136610470', 'Aa123456', True))
        
        # æ£€æŸ¥å¹¶æ·»åŠ is_usedå­—æ®µï¼ˆå…¼å®¹ç°æœ‰æ•°æ®åº“ï¼‰
        cursor.execute("PRAGMA table_info(suppliers)")
        columns = [column[1] for column in cursor.fetchall()]
        if 'is_used' not in columns:
            cursor.execute('ALTER TABLE suppliers ADD COLUMN is_used BOOLEAN DEFAULT FALSE')
            print("å·²æ·»åŠ is_usedå­—æ®µåˆ°suppliersè¡¨")
        
        # æ£€æŸ¥å¹¶æ·»åŠ ocr_recognition_statuså­—æ®µï¼ˆå…¼å®¹ç°æœ‰æ•°æ®åº“ï¼‰
        if 'ocr_recognition_status' not in columns:
            cursor.execute('ALTER TABLE suppliers ADD COLUMN ocr_recognition_status TEXT DEFAULT "pending"')
            print("å·²æ·»åŠ ocr_recognition_statuså­—æ®µåˆ°suppliersè¡¨")
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ•ˆç‡
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_ocr_recognition_status ON suppliers(ocr_recognition_status)')
            print("å·²ä¸ºocr_recognition_statuså­—æ®µåˆ›å»ºç´¢å¼•")
        
        conn.commit()
        conn.close()
        print("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def load_categories(self):
        """åŠ è½½åˆ†ç±»æ•°æ®"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„æ–‡ä»¶å
            possible_files = ['cary.json', 'cary. json']
            data = None
            
            for filename in possible_files:
                try:
                    with open(filename, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        print(f"æˆåŠŸåŠ è½½åˆ†ç±»æ–‡ä»¶: {filename}")
                        break
                except FileNotFoundError:
                    continue
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue
            
            if data and 'data' in data and 'tabs' in data['data']:
                categories = {}
                for tab in data['data']['tabs']:
                    categories[tab['tabId']] = tab['title']
                print(f"åŠ è½½äº† {len(categories)} ä¸ªåˆ†ç±»")
                return categories
            else:
                print("åˆ†ç±»æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                return {}
        except Exception as e:
            print(f"åŠ è½½åˆ†ç±»æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def build_category_search_url(self, category_id, page_no=1, page_size=20):
        """æ„å»ºåˆ†ç±»æœç´¢URL"""
        base_url = "https://insights.alibaba.com/openservice/gatewayService"
        
        params = {
            'endpoint': 'pc',
            'pageSize': str(page_size),
            'categoryIds': str(category_id),
            'pageNo': str(page_no),
            'modelId': '10300'
        }
        
        query_string = '&'.join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
        return f"{base_url}?{query_string}"

    def extract_suppliers_from_category_api(self, data_list):
        """ä»åˆ†ç±»APIæ•°æ®ä¸­æå–ä¾›åº”å•†ä¿¡æ¯"""
        suppliers = []
        
        for item in data_list:
            # ä»reviewsUrlæ„å»ºaction_url
            reviews_url = item.get('reviewsUrl', '')
            if reviews_url:
                # æå–åŸŸåéƒ¨åˆ†
                if reviews_url.startswith('//'):
                    reviews_url = reviews_url[2:]
                
                # è§£æURLè·å–åŸŸå
                parsed_url = urlparse(f"https://{reviews_url}")
                domain = parsed_url.netloc
                
                # æ„å»ºaction_url
                action_url = f"https://{domain}/zh_CN/company_profile.html?subpage=onsiteDetail"
            else:
                action_url = ''
            
            supplier = {
                'company_id': item.get('companyId', ''),
                'company_name': item.get('companyName', ''),
                'action_url': action_url,
                'country_code': '',  # æ–°APIä¸­æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                'city': '',  # æ–°APIä¸­æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                'gold_years': item.get('goldSupplierYearsText', ''),
                'verified_supplier': item.get('assessedSupplier', False),
                'is_factory': True,  # åˆ†ç±»æœç´¢éƒ½æ˜¯å·¥å‚
                'review_score': item.get('rate', ''),
                'review_count': item.get('reviews', ''),
                # æ–°å¢å­—æ®µ
                'company_on_time_shipping': item.get('companyOnTimeShipping', ''),
                'factory_size_text': item.get('factorySizeText', ''),
                'total_employees_text': item.get('totalEmployeesText', ''),
                'transaction_count_6months': item.get('transactionCountDuring6Months', ''),
                'transaction_gmv_6months_text': item.get('transactionGmvDuring6MonthsText', ''),
                'gold_supplier': item.get('goldSupplier', False),
                'trade_assurance': item.get('tradeAssurance', False),
                'response_time': item.get('responseTime', '')
            }
            
            if supplier['company_id']:  # åªæ·»åŠ æœ‰å…¬å¸IDçš„ä¾›åº”å•†
                suppliers.append(supplier)
        
        return suppliers

    def parse_proxy(self, proxy_string):
        """è§£æä»£ç†å­—ç¬¦ä¸²"""
        if not proxy_string:
            return None
        
        try:
            # è§£æä»£ç†æ ¼å¼: http://username:password@host:port
            if '@' in proxy_string:
                # æœ‰è®¤è¯ä¿¡æ¯çš„ä»£ç†
                auth_part, server_part = proxy_string.split('@', 1)
                protocol = auth_part.split('://')[0]
                auth = auth_part.split('://')[1]
                username, password = auth.split(':')
                host, port = server_part.split(':')
                
                return {
                    'host': host,
                    'port': int(port),
                    'username': username,
                    'password': password
                }
            else:
                # æ— è®¤è¯ä¿¡æ¯çš„ä»£ç†
                return None
        except Exception as e:
            print(f"ä»£ç†è§£æå¤±è´¥: {e}")
            return None
    
    async def fetch_with_proxy(self, url, proxy=None, session=None, is_html=False, check_ip=True, max_retries=3, log_callback=None):
        """ä½¿ç”¨ä»£ç†è·å–æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        # æ£€æŸ¥IPå˜åŒ–ï¼ˆå¯é€‰ï¼‰
        if check_ip:
            await self.check_ip_change(proxy)
        
        if is_html:
            # HTMLé¡µé¢çš„è¯·æ±‚å¤´ - ä½¿ç”¨éšæœºç§»åŠ¨ç«¯UA
            mobile_user_agents = [
                # iPhoneç³»åˆ—
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.7 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Mobile/15E148 Safari/604.1',
                
                # Samsung Galaxyç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-G996B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-A546B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-A546E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-S918N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Google Pixelç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 8 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; Pixel 6a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 7a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # OnePlusç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; OnePlus 9) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; OnePlus 11) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; OnePlus 10 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; OnePlus 12) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; OnePlus 9R) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Xiaomiç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; M2102J20SG) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; 2311DRK48C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; M2012K11AG) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; 2311DRK48I) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; M2102J20SI) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # OPPOç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; CPH2205) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; CPH2411) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; CPH2211) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; CPH2413) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; CPH2207) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # vivoç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; V2114) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; V2307) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; V2118) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; V2309) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; V2127) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Huaweiç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; HUAWEI P50 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; HUAWEI Mate 60 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; HUAWEI P60 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; HUAWEI Mate X5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; HUAWEI nova 11) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
            ]
            
            headers = {
                'User-Agent': random.choice(mobile_user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        else:
            # APIè¯·æ±‚å¤´ - ä¹Ÿä½¿ç”¨éšæœºç§»åŠ¨ç«¯UA
            mobile_user_agents = [
                # iPhoneç³»åˆ—
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.7 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Mobile/15E148 Safari/604.1',
                
                # Samsung Galaxyç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-G996B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-A546B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; SM-A546E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; SM-S918N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Google Pixelç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 8 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; Pixel 6a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; Pixel 7a) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # OnePlusç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; OnePlus 9) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; OnePlus 11) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; OnePlus 10 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; OnePlus 12) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; OnePlus 9R) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Xiaomiç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; M2102J20SG) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; 2311DRK48C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; M2012K11AG) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; 2311DRK48I) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; M2102J20SI) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # OPPOç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; CPH2205) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; CPH2411) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; CPH2211) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; CPH2413) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; CPH2207) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # vivoç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; V2114) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; V2307) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; V2118) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; V2309) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; V2127) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # Huaweiç³»åˆ—
                'Mozilla/5.0 (Linux; Android 13; HUAWEI P50 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; HUAWEI Mate 60 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; HUAWEI P60 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 14; HUAWEI Mate X5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; HUAWEI nova 11) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
            ]
            
            headers = {
                'User-Agent': random.choice(mobile_user_agents),
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': 'https://www.alibaba.com/',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site'
            }
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                if session:
                    # ä½¿ç”¨aiohttpå¼‚æ­¥è¯·æ±‚
                    if proxy:
                        # ä½¿ç”¨HTTPä»£ç†æ ¼å¼
                        proxy_url = f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}"
                        async with session.get(url, headers=headers, proxy=proxy_url) as response:
                            if response.status == 200:
                                if is_html:
                                    return await response.text()
                                else:
                                    return await response.json()
                            else:
                                self.log(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status} (å°è¯• {attempt + 1}/{max_retries})", "ERROR", log_callback)
                    else:
                        async with session.get(url, headers=headers) as response:
                            if response.status == 200:
                                if is_html:
                                    return await response.text()
                                else:
                                    return await response.json()
                            else:
                                self.log(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status} (å°è¯• {attempt + 1}/{max_retries})", "ERROR", log_callback)
                else:
                    # ä½¿ç”¨requestsåŒæ­¥è¯·æ±‚
                    proxies = None
                    if proxy:
                        # ä½¿ç”¨HTTPä»£ç†æ ¼å¼
                        proxies = {
                            'http': f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}",
                            'https': f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}"
                        }
                    response = requests.get(url, headers=headers, proxies=proxies, timeout=30)
                    if response.status_code == 200:
                        if is_html:
                            return response.text
                        else:
                            return response.json()
                    else:
                        self.log(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} (å°è¯• {attempt + 1}/{max_retries})", "ERROR", log_callback)
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š2ç§’ã€4ç§’ã€6ç§’
                    self.log(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO", log_callback)
                    await asyncio.sleep(wait_time)
                    
            except Exception as e:
                self.log(f"è¯·æ±‚å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {e}", "ERROR", log_callback)
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    self.log(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO", log_callback)
                    await asyncio.sleep(wait_time)
        
        self.log(f"è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})", "ERROR", log_callback)
        return None
    
    async def check_ip_change(self, proxy=None):
        """æ£€æŸ¥IPå˜åŒ–"""
        if not proxy:
            return
        
        # é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # è·å–å½“å‰IPï¼ˆä½¿ç”¨æœ€å¿«çš„icanhazip.comï¼‰
                ip_check_url = "https://icanhazip.com"
                proxies = None
                if proxy:
                    # ä½¿ç”¨HTTPä»£ç†æ ¼å¼
                    proxies = {
                        'http': f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}",
                        'https': f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}"
                    }
                
                response = requests.get(ip_check_url, proxies=proxies, timeout=10)
                if response.status_code == 200:
                    current_ip = response.text.strip()  # ç›´æ¥è·å–IPæ–‡æœ¬
                    print(f"å½“å‰IP: {current_ip}")
                    
                    # æ£€æŸ¥IPæ˜¯å¦å˜åŒ–
                    if hasattr(self, 'last_ip'):
                        if current_ip != self.last_ip:
                            print(f"IPå·²å˜åŒ–: {self.last_ip} -> {current_ip}")
                        else:
                            print(f"IPæœªå˜åŒ–: {current_ip}")
                    else:
                        print(f"é¦–æ¬¡è·å–IP: {current_ip}")
                    
                    self.last_ip = current_ip
                    return  # æˆåŠŸè·å–IPï¼Œé€€å‡ºé‡è¯•
                else:
                    print(f"IPæ£€æŸ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"æ£€æŸ¥IPå˜åŒ–æ—¶å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
        
        print("IPæ£€æŸ¥å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def build_search_url(self, keyword, page=1):
        """æ„å»ºæœç´¢URL"""
        base_url = "https://www.alibaba.com/search/api/supplierTextSearch"
        
        # URLå‚æ•°
        params = {
            'productQpKeywords': keyword,
            'cateIdLv1List': '66',
            'qpListData': '201758404,201757704,201334819,201762603,202221803',
            'supplierQpProductName': keyword.split()[0] if keyword else 'perfume',
            'query': keyword,
            'productAttributes': keyword.split()[0] if keyword else 'perfume',
            'pageSize': '20',
            'queryMachineTranslate': keyword,
            'productName': keyword,
            'intention': '',
            'queryProduct': f' {keyword}',
            'supplierAttributes': '',
            'requestId': f'AI_Web_2500000600257_{int(time.time() * 1000)}',
            'queryRaw': keyword,
            'supplierQpKeywords': keyword.replace(' ', '%2C'),
            'startTime': str(int(time.time() * 1000)),
            'page': str(page),
            'verifiedManufactory': 'true'
        }
        
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        query_string = '&'.join([f"{k}={urllib.parse.quote(str(v))}" for k, v in params.items()])
        return f"{base_url}?{query_string}"
    
    async def crawl_suppliers(self, keyword, pages=1, proxy=None, extract_licenses=False):
        """çˆ¬å–ä¾›åº”å•†æ•°æ®ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        return await self.crawl_suppliers_range(keyword, 1, pages, proxy, extract_licenses)
    
    async def crawl_suppliers_range(self, keyword, start_page=1, end_page=1, proxy=None, extract_licenses=False, skip_duplicates=True, log_callback=None, save_to_file=False, cache_file=None):
        """çˆ¬å–æŒ‡å®šé¡µé¢èŒƒå›´çš„ä¾›åº”å•†æ•°æ®"""
        try:
            all_suppliers = []
            total_saved = 0
            total_skipped = 0
            
            # åˆ›å»ºå¼‚æ­¥ä¼šè¯
            connector = aiohttp.TCPConnector(limit=10)
            timeout = aiohttp.ClientTimeout(total=30)
            
            # æ—¥å¿—è¾“å‡ºå‡½æ•°
            def log(message, level="INFO"):
                if log_callback:
                    log_callback(message, level)
                else:
                    print(message)
            
            log(f"ğŸš€ å¼€å§‹çˆ¬å–å…³é”®è¯: '{keyword}'ï¼Œé¡µé¢èŒƒå›´: {start_page}-{end_page}")
            if save_to_file:
                log(f"ğŸ“ æ•°æ®å°†ä¿å­˜åˆ°æ–‡ä»¶: {cache_file}")
            log("=" * 60)
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                for page in range(start_page, end_page + 1):
                    log(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ ({page}/{end_page})...")
                    
                    # æ„å»ºæœç´¢URL
                    search_url = self.build_search_url(keyword, page)
                    log(f"ğŸ”— è¯·æ±‚URL: {search_url}")
                    
                    try:
                        # ä½¿ç”¨ä»£ç†è¯·æ±‚
                        data = await self.fetch_with_proxy(search_url, proxy, session, log_callback=log_callback)
                        
                        if data.get('success') and 'model' in data and 'offers' in data['model']:
                            suppliers = self.extract_suppliers_from_api(data['model']['offers'])
                            log(f"ğŸ“¦ ç¬¬ {page} é¡µè·å–åˆ° {len(suppliers)} ä¸ªä¾›åº”å•†")
                            
                            if suppliers:
                                # ä¸ºæ¯ä¸ªä¾›åº”å•†æ·»åŠ å…³é”®è¯ä¿¡æ¯
                                for supplier in suppliers:
                                    supplier['keyword'] = keyword
                                    supplier['crawl_time'] = datetime.now().isoformat()
                                
                                if save_to_file:
                                    # ä¿å­˜åˆ°æ–‡ä»¶
                                    await self.save_suppliers_to_cache_file(suppliers, cache_file, log_callback=log)
                                    log(f"ğŸ“ ç¬¬ {page} é¡µæ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶")
                                else:
                                    # å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“
                                    log(f"ğŸ’¾ å¼€å§‹ä¿å­˜ç¬¬ {page} é¡µçš„ä¾›åº”å•†æ•°æ®...")
                                    page_saved = 0
                                    page_skipped = 0
                                    
                                    for i, supplier in enumerate(suppliers, 1):
                                        result = await self.save_single_supplier(supplier, skip_duplicates)
                                        if result:
                                            page_saved += 1
                                            total_saved += 1
                                        else:
                                            page_skipped += 1
                                            total_skipped += 1
                                        
                                        # æ˜¾ç¤ºè¿›åº¦
                                        log(f"  ğŸ“Š è¿›åº¦: {i}/{len(suppliers)} - æ–°å¢: {page_saved}, é‡å¤: {page_skipped}")
                                    
                                    log(f"âœ… ç¬¬ {page} é¡µä¿å­˜å®Œæˆ: æ–°å¢ {page_saved} ä¸ªï¼Œè·³è¿‡ {page_skipped} ä¸ªé‡å¤", "SUCCESS")
                                
                                all_suppliers.extend(suppliers)
                            
                            # æ‰“å°APIå“åº”çš„åˆ†é¡µä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
                            if 'model' in data and 'pagination' in data['model']:
                                pagination = data['model']['pagination']
                                if 'totalCount' in pagination:
                                    log(f"ğŸ“ˆ æ€»ä¾›åº”å•†æ•°: {pagination.get('totalCount', 'N/A')}")
                            
                        else:
                            log(f"âŒ ç¬¬ {page} é¡µAPIè¿”å›é”™è¯¯", "ERROR")
                            if data:
                                log(f"   é”™è¯¯è¯¦æƒ…: {data.get('message', 'æœªçŸ¥é”™è¯¯')}", "ERROR")
                    
                    except Exception as e:
                        log(f"âŒ çˆ¬å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}", "ERROR")
                        continue
                    
                    # é¡µé¢é—´å»¶è¿Ÿ
                    if page < end_page:
                        delay = random.uniform(0, 0)
                        log(f"â±ï¸  ç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€é¡µ...")
                        await asyncio.sleep(delay)
            
            # Excelæ–‡ä»¶æ›´æ–°å·²ç§»é™¤
            
            # æ€»ç»“
            log(f"ğŸ‰ çˆ¬å–å®Œæˆï¼", "SUCCESS")
            log(f"ğŸ“Š æ€»è®¡: æ–°å¢ {total_saved} ä¸ªä¾›åº”å•†ï¼Œè·³è¿‡ {total_skipped} ä¸ªé‡å¤")
            log("=" * 60)
            
            # å¦‚æœéœ€è¦æå–æ‰§ç…§å›¾ç‰‡
            if extract_licenses and all_suppliers:
                log("ğŸ” å¼€å§‹æå–ä¾›åº”å•†æ‰§ç…§å›¾ç‰‡...")
                # åˆ›å»ºæ–°çš„sessionç”¨äºè·å–HTMLé¡µé¢
                connector = aiohttp.TCPConnector(limit=10)
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(connector=connector, timeout=timeout) as html_session:
                    await self.extract_all_licenses(all_suppliers, proxy, html_session, log_callback)
            
            return all_suppliers
            
        except Exception as e:
            log(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}", "ERROR")
            return []
    
    async def crawl_suppliers_by_category(self, category_id, start_page=1, end_page=1, proxy=None, extract_licenses=False, skip_duplicates=True, save_path=None, log_callback=None):
        """æŒ‰åˆ†ç±»çˆ¬å–ä¾›åº”å•†"""
        # æ—¥å¿—è¾“å‡ºå‡½æ•°
        def log(message, level="INFO"):
            if log_callback:
                log_callback(message, level)
            else:
                print(message)
        
        if category_id not in self.categories:
            log(f"âŒ æœªçŸ¥çš„åˆ†ç±»ID: {category_id}", "ERROR")
            return []
        
        category_name = self.categories[category_id]
        all_suppliers = []
        total_saved = 0
        total_skipped = 0
        
        log(f"ğŸš€ å¼€å§‹çˆ¬å–åˆ†ç±»: {category_name} (ID: {category_id})ï¼Œé¡µé¢èŒƒå›´: {start_page}-{end_page}")
        log("=" * 60)
        
        # ä¸ºæ¯ä¸ªä¾›åº”å•†æ·»åŠ åˆ†ç±»ä¿¡æ¯
        connector = aiohttp.TCPConnector(limit=10)
        timeout = aiohttp.ClientTimeout(total=30)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            for page in range(start_page, end_page + 1):
                log(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ ({page}/{end_page})...")
                
                # æ„å»ºæœç´¢URL
                search_url = self.build_category_search_url(category_id, page)
                log(f"ğŸ”— è¯·æ±‚URL: {search_url}")
                
                try:
                    # è¯·æ±‚æ•°æ®
                    data = await self.fetch_with_proxy(search_url, proxy, session, log_callback=log_callback)
                    
                    if data and data.get('code') == 200 and 'data' in data and 'list' in data['data']:
                        suppliers = self.extract_suppliers_from_category_api(data['data']['list'])
                        log(f"ğŸ“¦ ç¬¬ {page} é¡µè·å–åˆ° {len(suppliers)} ä¸ªä¾›åº”å•†")
                        
                        # ä¸ºæ¯ä¸ªä¾›åº”å•†æ·»åŠ åˆ†ç±»ä¿¡æ¯
                        for supplier in suppliers:
                            supplier['category_id'] = category_id
                            supplier['category_name'] = category_name
                            supplier['save_path'] = save_path
                        
                        # å®æ—¶ä¿å­˜æ¯ä¸ªä¾›åº”å•†
                        if suppliers:
                            log(f"ğŸ’¾ å¼€å§‹ä¿å­˜ç¬¬ {page} é¡µçš„ä¾›åº”å•†æ•°æ®...")
                            page_saved = 0
                            page_skipped = 0
                            
                            for i, supplier in enumerate(suppliers, 1):
                                result = await self.save_single_supplier(supplier, skip_duplicates)
                                if result:
                                    page_saved += 1
                                    total_saved += 1
                                else:
                                    page_skipped += 1
                                    total_skipped += 1
                                
                                # æ˜¾ç¤ºè¿›åº¦
                                log(f"  ğŸ“Š è¿›åº¦: {i}/{len(suppliers)} - æ–°å¢: {page_saved}, é‡å¤: {page_skipped}")
                            
                            log(f"âœ… ç¬¬ {page} é¡µä¿å­˜å®Œæˆ: æ–°å¢ {page_saved} ä¸ªï¼Œè·³è¿‡ {page_skipped} ä¸ªé‡å¤", "SUCCESS")
                            all_suppliers.extend(suppliers)
                        
                        # æ‰“å°åˆ†é¡µä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
                        if 'page' in data['data']:
                            page_info = data['data']['page']
                            if 'totalCount' in page_info:
                                log(f"ğŸ“ˆ æ€»ä¾›åº”å•†æ•°: {page_info.get('totalCount', 'N/A')}")
                        
                    else:
                        log(f"âŒ ç¬¬ {page} é¡µAPIè¿”å›é”™è¯¯", "ERROR")
                        if data:
                            log(f"   é”™è¯¯è¯¦æƒ…: {data.get('message', 'æœªçŸ¥é”™è¯¯')}", "ERROR")
                
                except Exception as e:
                    log(f"âŒ çˆ¬å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}", "ERROR")
                    continue
                
                # å»¶è¿Ÿ
                if page < end_page:
                    delay = random.uniform(0, 0)
                    log(f"â±ï¸  ç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€é¡µ...")
                    await asyncio.sleep(delay)
        
        # Excelæ–‡ä»¶æ›´æ–°å·²ç§»é™¤
        
        # æ€»ç»“
        log(f"ğŸ‰ åˆ†ç±»çˆ¬å–å®Œæˆï¼", "SUCCESS")
        log(f"ğŸ“Š æ€»è®¡: æ–°å¢ {total_saved} ä¸ªä¾›åº”å•†ï¼Œè·³è¿‡ {total_skipped} ä¸ªé‡å¤", "SUCCESS")
        log("=" * 60)
        
        # å¦‚æœéœ€è¦æå–æ‰§ç…§
        if extract_licenses and all_suppliers:
            log("å¼€å§‹æå–æ‰§ç…§ä¿¡æ¯...")
            await self.extract_licenses_from_database(proxy)
        
        return all_suppliers
    
    async def save_single_supplier_to_category(self, company_id, company_name, licenses, license_info, category_id, category_name):
        """ä¿å­˜å•ä¸ªä¾›åº”å•†åˆ°å¯¹åº”åˆ†ç±»ç›®å½•"""
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_path = "./license_files"
            category_dir = os.path.join(save_path, f"{category_id}_{category_name}")
            os.makedirs(category_dir, exist_ok=True)
            
            # åˆ›å»ºä¾›åº”å•†ç›®å½•
            safe_name = company_name.replace('/', '_').replace('\\', '_').replace(':', '_')[:50]
            supplier_dir = os.path.join(category_dir, safe_name)
            os.makedirs(supplier_dir, exist_ok=True)
            
            # ä¿å­˜æ‰§ç…§ä¿¡æ¯
            if license_info:
                info_file = os.path.join(supplier_dir, "æ‰§ç…§ä¿¡æ¯.txt")
                with open(info_file, 'w', encoding='utf-8') as f:
                    f.write(f"ä¾›åº”å•†: {company_name}\n")
                    f.write(f"å…¬å¸ID: {company_id}\n")
                    f.write(f"åˆ†ç±»: {category_name}\n")
                    f.write("=" * 50 + "\n")
                    for field_name, field_value in license_info.items():
                        f.write(f"{field_name}: {field_value}\n")
            
            # ä¸‹è½½æ‰§ç…§å›¾ç‰‡
            if licenses:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    for i, license_item in enumerate(licenses):
                        try:
                            async with session.get(license_item['url']) as response:
                                if response.status == 200:
                                    content = await response.read()
                                    file_ext = license_item['url'].split('.')[-1] if '.' in license_item['url'] else 'jpg'
                                    img_file = os.path.join(supplier_dir, f"æ‰§ç…§å›¾ç‰‡_{i+1}.{file_ext}")
                                    with open(img_file, 'wb') as f:
                                        f.write(content)
                        except Exception as e:
                            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {license_item['url']} - {e}")
            
            # æ›´æ–°æ•°æ®åº“ä¸­çš„save_pathå­—æ®µ
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute('UPDATE suppliers SET save_path = ? WHERE company_id = ?', (supplier_dir, company_id))
                conn.commit()
                conn.close()
            except Exception as db_e:
                print(f"æ›´æ–°æ•°æ®åº“save_pathå¤±è´¥: {db_e}")
            
            print(f"  - {company_name}: å·²ä¿å­˜åˆ°åˆ†ç±»ç›®å½•: {supplier_dir}")
            return supplier_dir
            
        except Exception as e:
            print(f"ä¿å­˜ä¾›åº”å•†æ–‡ä»¶å¤±è´¥: {company_name} - {e}")
            return None
    
    def generate_save_path(self, supplier):
        """ç”Ÿæˆç»Ÿä¸€çš„ä¿å­˜è·¯å¾„"""
        base_path = "./license_files"
        
        # æ¸…ç†å…¬å¸åä½œä¸ºæ–‡ä»¶å¤¹å
        safe_company_name = supplier['company_name'].replace('/', '_').replace('\\', '_').replace(':', '_')[:50]
        
        # æ ¹æ®å…³é”®è¯æˆ–åˆ†ç±»ç”Ÿæˆè·¯å¾„
        if 'keyword' in supplier and supplier['keyword']:
            # å…³é”®è¯æœç´¢ï¼š./license_files/{å…³é”®è¯}/{å…¬å¸å}/
            safe_keyword = supplier['keyword'].replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            return os.path.join(base_path, safe_keyword, safe_company_name)
        elif 'category_id' in supplier and supplier['category_id'] and 'category_name' in supplier and supplier['category_name']:
            # åˆ†ç±»æœç´¢ï¼š./license_files/{åˆ†ç±»ID}_{åˆ†ç±»å}/{å…¬å¸å}/
            safe_category_name = supplier['category_name'].replace('/', '_').replace('\\', '_').replace(':', '_')
            category_folder = f"{supplier['category_id']}_{safe_category_name}"
            return os.path.join(base_path, category_folder, safe_company_name)
        else:
            # é»˜è®¤è·¯å¾„ï¼š./license_files/{å…¬å¸å}/
            return os.path.join(base_path, safe_company_name)
    
    async def save_single_supplier_to_keyword(self, company_id, company_name, licenses, license_info, keyword):
        """ä¿å­˜å•ä¸ªä¾›åº”å•†åˆ°å…³é”®è¯ç›®å½•"""
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_path = "./license_files"
            # æ¸…ç†å…³é”®è¯ä½œä¸ºæ–‡ä»¶å¤¹å
            safe_keyword = keyword.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            keyword_dir = os.path.join(save_path, safe_keyword)
            os.makedirs(keyword_dir, exist_ok=True)
            
            # åˆ›å»ºä¾›åº”å•†ç›®å½•
            safe_name = company_name.replace('/', '_').replace('\\', '_').replace(':', '_')[:50]
            supplier_dir = os.path.join(keyword_dir, safe_name)
            os.makedirs(supplier_dir, exist_ok=True)
            
            # ä¿å­˜æ‰§ç…§ä¿¡æ¯
            if license_info:
                info_file = os.path.join(supplier_dir, "æ‰§ç…§ä¿¡æ¯.txt")
                with open(info_file, 'w', encoding='utf-8') as f:
                    f.write(f"ä¾›åº”å•†: {company_name}\n")
                    f.write(f"å…¬å¸ID: {company_id}\n")
                    f.write(f"å…³é”®è¯: {keyword}\n")
                    f.write("=" * 50 + "\n")
                    for field_name, field_value in license_info.items():
                        f.write(f"{field_name}: {field_value}\n")
            
            # ä¸‹è½½æ‰§ç…§å›¾ç‰‡
            if licenses:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    for i, license_item in enumerate(licenses):
                        try:
                            async with session.get(license_item['url']) as response:
                                if response.status == 200:
                                    content = await response.read()
                                    file_ext = license_item['url'].split('.')[-1] if '.' in license_item['url'] else 'jpg'
                                    img_file = os.path.join(supplier_dir, f"æ‰§ç…§å›¾ç‰‡_{i+1}.{file_ext}")
                                    with open(img_file, 'wb') as f:
                                        f.write(content)
                        except Exception as e:
                            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {license_item['url']} - {e}")
            
            print(f"  - {company_name}: å·²ä¿å­˜åˆ°å…³é”®è¯ç›®å½•: {supplier_dir}")
            return supplier_dir
            
        except Exception as e:
            print(f"ä¿å­˜ä¾›åº”å•†æ–‡ä»¶å¤±è´¥: {company_name} - {e}")
            return None
    
    async def save_suppliers_by_category(self, category_id, save_path=None):
        """æŒ‰åˆ†ç±»ä¿å­˜ä¾›åº”å•†æ•°æ®åˆ°æ–‡ä»¶"""
        if category_id not in self.categories:
            print(f"æœªçŸ¥çš„åˆ†ç±»ID: {category_id}")
            return
        
        category_name = self.categories[category_id]
        print(f"å¼€å§‹ä¿å­˜åˆ†ç±»: {category_name} çš„ä¾›åº”å•†æ•°æ®")
        
        # ä»æ•°æ®åº“è·å–è¯¥åˆ†ç±»çš„ä¾›åº”å•†
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT company_id, company_name, action_url, country_code, city, 
                   gold_years, verified_supplier, is_factory, review_score, review_count,
                   company_on_time_shipping, factory_size_text, total_employees_text,
                   transaction_count_6months, transaction_gmv_6months_text, gold_supplier,
                   trade_assurance, response_time, save_path
            FROM suppliers 
            WHERE category_id = ?
            ORDER BY created_at DESC
        ''', (category_id,))
        
        suppliers = cursor.fetchall()
        conn.close()
        
        if not suppliers:
            print(f"åˆ†ç±» {category_name} æ²¡æœ‰ä¾›åº”å•†æ•°æ®")
            return
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        if not save_path:
            save_path = "./license_files"
        
        category_dir = os.path.join(save_path, f"{category_id}_{category_name}")
        os.makedirs(category_dir, exist_ok=True)
        
        # ä¿å­˜Excelæ–‡ä»¶
        import pandas as pd
        
        # è½¬æ¢ä¸ºDataFrame
        df_data = []
        for supplier in suppliers:
            df_data.append({
                'company_id': supplier[0],
                'company_name': supplier[1],
                'action_url': supplier[2],
                'country_code': supplier[3],
                'city': supplier[4],
                'gold_years': supplier[5],
                'verified_supplier': supplier[6],
                'is_factory': supplier[7],
                'review_score': supplier[8],
                'review_count': supplier[9],
                'company_on_time_shipping': supplier[10],
                'factory_size_text': supplier[11],
                'total_employees_text': supplier[12],
                'transaction_count_6months': supplier[13],
                'transaction_gmv_6months_text': supplier[14],
                'gold_supplier': supplier[15],
                'trade_assurance': supplier[16],
                'response_time': supplier[17],
                'save_path': supplier[18]
            })
        
        df = pd.DataFrame(df_data)
        excel_file = os.path.join(category_dir, f"{category_name}_ä¾›åº”å•†æ•°æ®.xlsx")
        df.to_excel(excel_file, index=False)
        
        # ä¿å­˜æ‰§ç…§å›¾ç‰‡å’Œä¿¡æ¯
        for supplier in suppliers:
            company_id, company_name = supplier[0], supplier[1]
            
            # è·å–æ‰§ç…§å›¾ç‰‡
            cursor.execute('SELECT license_url FROM licenses WHERE company_id = ?', (company_id,))
            licenses = cursor.fetchall()
            
            # è·å–æ‰§ç…§ä¿¡æ¯
            cursor.execute('SELECT field_name, field_value FROM license_info WHERE company_id = ?', (company_id,))
            license_info = cursor.fetchall()
            
            if licenses or license_info:
                # åˆ›å»ºä¾›åº”å•†ç›®å½•
                safe_name = company_name.replace('/', '_').replace('\\', '_').replace(':', '_')[:50]
                supplier_dir = os.path.join(category_dir, safe_name)
                os.makedirs(supplier_dir, exist_ok=True)
                
                # ä¿å­˜æ‰§ç…§ä¿¡æ¯
                if license_info:
                    info_file = os.path.join(supplier_dir, "æ‰§ç…§ä¿¡æ¯.txt")
                    with open(info_file, 'w', encoding='utf-8') as f:
                        f.write(f"ä¾›åº”å•†: {company_name}\n")
                        f.write(f"å…¬å¸ID: {company_id}\n")
                        f.write("=" * 50 + "\n")
                        for field_name, field_value in license_info:
                            f.write(f"{field_name}: {field_value}\n")
                
                # ä¸‹è½½æ‰§ç…§å›¾ç‰‡
                if licenses:
                    import aiohttp
                    async with aiohttp.ClientSession() as session:
                        for i, (license_url,) in enumerate(licenses):
                            try:
                                async with session.get(license_url) as response:
                                    if response.status == 200:
                                        content = await response.read()
                                        file_ext = license_url.split('.')[-1] if '.' in license_url else 'jpg'
                                        img_file = os.path.join(supplier_dir, f"æ‰§ç…§å›¾ç‰‡_{i+1}.{file_ext}")
                                        with open(img_file, 'wb') as f:
                                            f.write(content)
                            except Exception as e:
                                print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {license_url} - {e}")
        
        print(f"åˆ†ç±» {category_name} çš„æ•°æ®å·²ä¿å­˜åˆ°: {category_dir}")
        return category_dir
    
    def extract_suppliers_from_api(self, offers):
        """ä»APIæ•°æ®ä¸­æå–ä¾›åº”å•†ä¿¡æ¯"""
        suppliers = []
        
        for offer in offers:
            # å¤„ç†action_urlï¼Œç¡®ä¿åŒ…å«subpageå‚æ•°
            action_url = f"https:{offer.get('action', '')}" if offer.get('action', '').startswith('//') else offer.get('action', '')
            
            # æ·»åŠ subpageå‚æ•°
            if action_url:
                if '?' in action_url:
                    action_url += '&subpage=onsiteDetail'
                else:
                    action_url += '?subpage=onsiteDetail'
            
            supplier = {
                'company_id': offer.get('companyId', ''),
                'company_name': offer.get('companyName', ''),
                'action_url': action_url,
                'country_code': offer.get('countryCode', ''),
                'city': offer.get('city', ''),
                'gold_years': offer.get('goldYears', ''),
                'verified_supplier': offer.get('verifiedSupplier', False),
                'is_factory': offer.get('isFactory', False),
                'review_score': offer.get('reviewScore', ''),
                'review_count': offer.get('reviewCount', 0)
            }
            
            if supplier['company_id']:  # åªæ·»åŠ æœ‰å…¬å¸IDçš„ä¾›åº”å•†
                suppliers.append(supplier)
        
        return suppliers
    
    async def save_single_supplier(self, supplier, skip_duplicates=True):
        """å®æ—¶ä¿å­˜å•ä¸ªä¾›åº”å•†æ•°æ®"""
        max_retries = 3
        retry_delay = 0.5  # 500ms
        
        for attempt in range(max_retries):
            conn = None
            try:
                # è®¾ç½®æ•°æ®åº“è¿æ¥è¶…æ—¶å’ŒWALæ¨¡å¼
                conn = sqlite3.connect(self.db_path, timeout=10.0)
                conn.execute('PRAGMA journal_mode=WAL')
                conn.execute('PRAGMA synchronous=NORMAL')
                conn.execute('PRAGMA busy_timeout=10000')  # 10ç§’è¶…æ—¶
                cursor = conn.cursor()
                
                # å¼€å§‹äº‹åŠ¡
                conn.execute('BEGIN IMMEDIATE')
                
                if skip_duplicates:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    cursor.execute('SELECT company_id FROM suppliers WHERE company_id = ?', (supplier['company_id'],))
                    existing = cursor.fetchone()
                    
                    if existing:
                        print(f"  âœ“ è·³è¿‡é‡å¤ä¾›åº”å•†: {supplier['company_name']} (ID: {supplier['company_id']})")
                        conn.rollback()
                        return False
                
                # ç”Ÿæˆä¿å­˜è·¯å¾„
                save_path = self.generate_save_path(supplier)
                supplier['save_path'] = save_path
                
                cursor.execute('''
                    INSERT INTO suppliers (company_id, company_name, action_url, country_code, 
                                       city, gold_years, verified_supplier, is_factory, 
                                       review_score, review_count, company_on_time_shipping,
                                       factory_size_text, total_employees_text, transaction_count_6months,
                                       transaction_gmv_6months_text, gold_supplier, trade_assurance, response_time,
                                       category_id, category_name, save_path)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    supplier['company_id'],
                    supplier['company_name'],
                    supplier['action_url'],
                    supplier['country_code'],
                    supplier['city'],
                    supplier['gold_years'],
                    supplier['verified_supplier'],
                    supplier['is_factory'],
                    supplier['review_score'],
                    supplier['review_count'],
                    supplier.get('company_on_time_shipping', ''),
                    supplier.get('factory_size_text', ''),
                    supplier.get('total_employees_text', ''),
                    supplier.get('transaction_count_6months', ''),
                    supplier.get('transaction_gmv_6months_text', ''),
                    supplier.get('gold_supplier', False),
                    supplier.get('trade_assurance', False),
                    supplier.get('response_time', ''),
                    supplier.get('category_id', ''),
                    supplier.get('category_name', ''),
                    save_path
                ))
                
                conn.commit()
                print(f"  âœ“ æˆåŠŸä¿å­˜ä¾›åº”å•†: {supplier['company_name']} (ID: {supplier['company_id']})")
                return True
                
            except sqlite3.OperationalError as e:
                if conn:
                    conn.rollback()
                
                if "database is locked" in str(e) and attempt < max_retries - 1:
                    print(f"  âš ï¸ æ•°æ®åº“é”å®šï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {supplier['company_name']}")
                    await asyncio.sleep(retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    print(f"  âœ— ä¿å­˜ä¾›åº”å•†å¤±è´¥: {supplier['company_name']} - {e}")
                    return False
            except Exception as e:
                if conn:
                    conn.rollback()
                print(f"  âœ— ä¿å­˜ä¾›åº”å•†å¤±è´¥: {supplier['company_name']} - {e}")
                return False
            finally:
                if conn:
                    conn.close()
        
        return False
    
    async def save_suppliers(self, suppliers, skip_duplicates=True):
        """æ‰¹é‡ä¿å­˜ä¾›åº”å•†æ•°æ®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        if not suppliers:
            print("æ²¡æœ‰ä¾›åº”å•†æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        saved_count = 0
        skipped_count = 0
        
        for supplier in suppliers:
            result = await self.save_single_supplier(supplier, skip_duplicates)
            if result:
                saved_count += 1
            else:
                skipped_count += 1
        
        # Excelæ–‡ä»¶æ›´æ–°å·²ç§»é™¤
        
        if skip_duplicates:
            print(f"æ‰¹é‡ä¿å­˜å®Œæˆ: {saved_count} ä¸ªæ–°å¢ï¼Œ{skipped_count} ä¸ªé‡å¤")
        else:
            print(f"æ‰¹é‡ä¿å­˜å®Œæˆ: {saved_count} ä¸ªä¾›åº”å•†æ•°æ®")
    
    async def check_image_size(self, url, base_name, file_ext):
        """å¼‚æ­¥æ£€æŸ¥å•ä¸ªå›¾ç‰‡å¤§å°"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        content_length = response.headers.get('content-length')
                        if content_length:
                            file_size = int(content_length)
                            if file_size >= 20 * 1024:  # 20KB = 20 * 1024 bytes
                                license_item = {
                                    'name': f"{base_name.split('.')[0]}.{file_ext}",
                                    'type': 'img',
                                    'url': url,
                                    'fileId': base_name,
                                    'file_size': file_size
                                }
                                print(f"ä¿ç•™æ‰§ç…§å›¾ç‰‡: {url} (å¤§å°: {file_size} bytes)")
                                return license_item
                            else:
                                print(f"å¿½ç•¥å°å›¾ç‰‡: {url} (å¤§å°: {file_size} bytes, å°äº20KB)")
                                return None
                        else:
                            # å¦‚æœæ²¡æœ‰content-lengthå¤´ï¼Œä¿ç•™å›¾ç‰‡ï¼Œè®¾ç½®é»˜è®¤å¤§å°ä¸º0
                            license_item = {
                                'name': f"{base_name.split('.')[0]}.{file_ext}",
                                'type': 'img',
                                'url': url,
                                'fileId': base_name,
                                'file_size': 0
                            }
                            print(f"ä¿ç•™æ‰§ç…§å›¾ç‰‡: {url} (æ— æ³•è·å–å¤§å°)")
                            return license_item
                    else:
                        print(f"æ— æ³•è®¿é—®å›¾ç‰‡: {url} (çŠ¶æ€ç : {response.status})")
                        return None
        except Exception as e:
            print(f"æ£€æŸ¥å›¾ç‰‡å¤§å°æ—¶å‡ºé”™: {url} - {e}")
            # å‡ºé”™æ—¶ä¿ç•™å›¾ç‰‡
            license_item = {
                'name': f"{base_name.split('.')[0]}.{file_ext}",
                'type': 'img',
                'url': url,
                'fileId': base_name
            }
            return license_item

    async def extract_licenses_from_html(self, html_content):
        """ä»HTMLå†…å®¹ä¸­æå–æ‰§ç…§å›¾ç‰‡ä¿¡æ¯ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        try:
            # æœç´¢æ‰€æœ‰æ‰§ç…§å›¾ç‰‡URLï¼ˆæ”¯æŒjpgå’Œpngæ ¼å¼ï¼‰
            url_pattern = r'https://sc04\.alicdn\.com/kf/([^"]+\.(?:jpg|png))'
            matches = re.findall(url_pattern, html_content)
            
            if matches:
                # å»é‡å¤„ç†ï¼šåªä¿ç•™åŸå§‹å°ºå¯¸çš„å›¾ç‰‡ï¼ˆä¸åŒ…å«å°ºå¯¸åç¼€ï¼‰
                unique_urls = {}
                for file_id in matches:
                    # æå–åŸºç¡€æ–‡ä»¶åï¼ˆå»é™¤å°ºå¯¸åç¼€ï¼‰
                    base_name = file_id
                    file_ext = file_id.split('.')[-1]  # è·å–æ–‡ä»¶æ‰©å±•å
                    
                    if '_' in file_id and any(size in file_id for size in ['_50x50', '_80x80', '_100x100', '_120x120', '_200x200', '_250x250', '_350x350']):
                        # å¦‚æœæœ‰å°ºå¯¸åç¼€ï¼Œå°è¯•æ‰¾åˆ°åŸå§‹æ–‡ä»¶å
                        for size in ['_50x50', '_80x80', '_100x100', '_120x120', '_200x200', '_250x250', '_350x350']:
                            if file_id.endswith(size + '.' + file_ext):
                                base_name = file_id.replace(size + '.' + file_ext, '.' + file_ext)
                                break
                    
                    # åªä¿ç•™åŸå§‹å°ºå¯¸çš„å›¾ç‰‡
                    if not any(size in base_name for size in ['_50x50', '_80x80', '_100x100', '_120x120', '_200x200', '_250x250', '_350x350']):
                        url = f"https://sc04.alicdn.com/kf/{base_name}"
                        unique_urls[base_name] = (url, base_name, file_ext)
                
                # å¹¶å‘æ£€æŸ¥å›¾ç‰‡å¤§å°
                tasks = []
                for base_name, (url, base_name, file_ext) in unique_urls.items():
                    task = self.check_image_size(url, base_name, file_ext)
                    tasks.append(task)
                
                # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹10ä¸ªï¼ˆå¢åŠ å¹¶å‘ï¼‰
                batch_size = 10
                all_licenses = []
                
                for i in range(0, len(tasks), batch_size):
                    batch_tasks = tasks[i:i + batch_size]
                    results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                    
                    for result in results:
                        if isinstance(result, Exception):
                            print(f"æ£€æŸ¥å›¾ç‰‡å¤§å°æ—¶å‡ºé”™: {result}")
                        elif result is not None:
                            all_licenses.append(result)
                
                # åªä¿ç•™æœ€å¤§çš„ä¸€å¼ å›¾ç‰‡
                if all_licenses:
                    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œé€‰æ‹©æœ€å¤§çš„ä¸€å¼ 
                    largest_license = max(all_licenses, key=lambda x: x.get('file_size', 0))
                    print(f"æ‰¾åˆ° {len(all_licenses)} ä¸ªæœ‰æ•ˆæ‰§ç…§å›¾ç‰‡ï¼Œä¿ç•™æœ€å¤§çš„ä¸€å¼ : {largest_license['url']} (å¤§å°: {largest_license.get('file_size', 0)} bytes)")
                    return [largest_license]
                else:
                    print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ‰§ç…§å›¾ç‰‡")
                    return []
            else:
                print("æœªæ‰¾åˆ°æ‰§ç…§å›¾ç‰‡")
                return []
                
        except Exception as e:
            print(f"æå–æ‰§ç…§å›¾ç‰‡å¤±è´¥: {e}")
            return []
    
    def extract_license_info_from_html(self, html_content):
        """ä»HTMLå†…å®¹ä¸­æå–æ‰§ç…§è¯¦ç»†ä¿¡æ¯"""
        try:
            # æå–è¥ä¸šæ‰§ç…§ä¿¡æ¯çš„æ­£åˆ™è¡¨è¾¾å¼
            info_patterns = {
                'registration_no': r'<span>Registration No\.</span>\s*:\s*([^<]+)',
                'company_name': r'<span>Company Name</span>\s*:\s*([^<]+)',
                'date_of_issue': r'<span>Date of Issue</span>\s*:\s*([^<]+)',
                'date_of_expiry': r'<span>Date of Expiry</span>\s*:\s*([^<]+)',
                'registered_capital': r'<span>Registered Capital</span>\s*:\s*([^<]+)',
                'country_territory': r'<span>Country/Territory</span>\s*:\s*([^<]+)',
                'registered_address': r'<span>Registered address</span>\s*:\s*([^<]+)',
                'year_established': r'<span>Year Established</span>\s*:\s*([^<]+)',
                'legal_form': r'<span>Legal Form</span>\s*:\s*([^<]+)',
                'legal_representative': r'<span>Legal Representative</span>\s*:\s*([^<]+)'
            }
            
            license_info = {}
            for field, pattern in info_patterns.items():
                match = re.search(pattern, html_content)
                if match:
                    license_info[field] = match.group(1).strip()
                else:
                    license_info[field] = ''
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†ä»»ä½•ä¿¡æ¯
            if any(license_info.values()):
                print(f"æ‰¾åˆ°æ‰§ç…§ä¿¡æ¯: {len([v for v in license_info.values() if v])} ä¸ªå­—æ®µ")
                return license_info
            else:
                print("æœªæ‰¾åˆ°æ‰§ç…§ä¿¡æ¯")
                return None
                
        except Exception as e:
            print(f"æå–æ‰§ç…§ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def fetch_supplier_page(self, action_url, proxy=None, session=None, check_ip=False, log_callback=None):
        """è·å–ä¾›åº”å•†é¡µé¢HTML"""
        try:
            # ç¡®ä¿URLåŒ…å«subpageå‚æ•°
            if 'subpage=onsiteDetail' not in action_url:
                if '?' in action_url:
                    actual_url = action_url + '&subpage=onsiteDetail'
                else:
                    actual_url = action_url + '?subpage=onsiteDetail'
            else:
                actual_url = action_url
            
            self.log(f"è¯·æ±‚ä¾›åº”å•†é¡µé¢: {actual_url}", "INFO", log_callback)
            
            # ä½¿ç”¨HTMLè¯·æ±‚å¤´è·å–é¡µé¢ï¼ˆå¯æ§åˆ¶IPæ£€æŸ¥ï¼‰
            html_content = await self.fetch_with_proxy(actual_url, proxy, session, is_html=True, check_ip=check_ip, log_callback=log_callback)
            
            if html_content:
                self.log(f"æˆåŠŸè·å–é¡µé¢ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦", "SUCCESS", log_callback)
                return html_content
            else:
                self.log("è·å–é¡µé¢å¤±è´¥", "ERROR", log_callback)
                return None
                
        except Exception as e:
            self.log(f"è·å–ä¾›åº”å•†é¡µé¢å¤±è´¥: {e}", "ERROR", log_callback)
            return None
    
    async def extract_all_licenses(self, suppliers, proxy=None, session=None, log_callback=None):
        """æå–æ‰€æœ‰ä¾›åº”å•†çš„æ‰§ç…§å›¾ç‰‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for i, supplier in enumerate(suppliers, 1):
                self.log(f"å¤„ç†ç¬¬ {i}/{len(suppliers)} ä¸ªä¾›åº”å•†: {supplier['company_name']}", "INFO", log_callback)
                
                # è·å–ä¾›åº”å•†é¡µé¢HTMLï¼ˆå¯ç”¨IPæ£€æµ‹ï¼‰
                html_content = await self.fetch_supplier_page(supplier['action_url'], proxy, session, check_ip=True, log_callback=log_callback)
                
                if html_content:
                    # æå–æ‰§ç…§å›¾ç‰‡
                    licenses = await self.extract_licenses_from_html(html_content)
                    
                    if licenses:
                        # ä¿å­˜æ‰§ç…§å›¾ç‰‡åˆ°æ•°æ®åº“
                        for license_item in licenses:
                            cursor.execute('''
                                INSERT INTO licenses (supplier_id, license_name, license_url, file_id)
                                VALUES (?, ?, ?, ?)
                            ''', (
                                supplier['company_id'],  # ä½¿ç”¨company_idä½œä¸ºsupplier_id
                                license_item['name'],
                                license_item['url'],
                                license_item['fileId']
                            ))
                        
                        print(f"  - æ‰¾åˆ° {len(licenses)} ä¸ªæ‰§ç…§å›¾ç‰‡")
                    else:
                        print(f"  - æœªæ‰¾åˆ°æ‰§ç…§å›¾ç‰‡")
                
                # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(suppliers):
                    await asyncio.sleep(random.uniform(0.5, 1))
            
            conn.commit()
            conn.close()
            print("æ‰§ç…§å›¾ç‰‡æå–å®Œæˆ")
            
        except Exception as e:
            print(f"æå–æ‰§ç…§å›¾ç‰‡è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            if 'conn' in locals():
                conn.close()
    
    async def extract_licenses_from_database(self, proxy=None):
        """ä»æ•°æ®åº“ä¸­çš„ä¾›åº”å•†æå–æ‰§ç…§å›¾ç‰‡ - ä½¿ç”¨10ä¸ªå¹¶å‘çº¿ç¨‹"""
        try:
            # è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰ä¾›åº”å•†
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT company_id, company_name, action_url 
                FROM suppliers 
                WHERE (license_extracted = FALSE OR license_extracted IS NULL)
                  AND (skip_extraction = FALSE OR skip_extraction IS NULL)
                ORDER BY created_at DESC
            ''')
            
            suppliers = cursor.fetchall()
            conn.close()
            
            if not suppliers:
                print("æ•°æ®åº“ä¸­æ²¡æœ‰ä¾›åº”å•†æ•°æ®")
                return 0
            
            print(f"ä»æ•°æ®åº“è·å–åˆ° {len(suppliers)} ä¸ªä¾›åº”å•†ï¼Œä½¿ç”¨5ä¸ªå¹¶å‘çº¿ç¨‹å¤„ç†")
            
            # åˆ›å»ºå¼‚æ­¥ä¼šè¯ - ä¼˜åŒ–å¹¶å‘è®¾ç½®
            connector = aiohttp.TCPConnector(limit=20)  # ä»60å‡å°‘åˆ°20
            timeout = aiohttp.ClientTimeout(total=15)  # ä»10å¢åŠ åˆ°15ç§’
            
            processed_count = 0
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                # å°†ä¾›åº”å•†åˆ†æˆ5ä¸ªä¸€ç»„ - å‡å°‘æ‰¹æ¬¡å¤§å°
                batch_size = 5
                for i in range(0, len(suppliers), batch_size):
                    batch = suppliers[i:i + batch_size]
                    print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(suppliers) + batch_size - 1)//batch_size}: {len(batch)} ä¸ªä¾›åº”å•†")
                    
                    # åˆ›å»º5ä¸ªå¹¶å‘ä»»åŠ¡
                    tasks = []
                    for company_id, company_name, action_url in batch:
                        task = self.process_single_supplier(company_id, company_name, action_url, proxy, session)
                        tasks.append(task)
                    
                    # å¹¶å‘æ‰§è¡Œï¼Œä¸ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    # ä½¿ç”¨asyncio.create_taskæ¥åˆ›å»ºä»»åŠ¡ï¼Œç„¶åç«‹å³å¼€å§‹ä¸‹ä¸€æ‰¹
                    running_tasks = []
                    for task in tasks:
                        running_tasks.append(asyncio.create_task(task))
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡å®Œæˆï¼ˆä¸é˜»å¡ä¸‹ä¸€æ‰¹ï¼‰
                    for task in running_tasks:
                        try:
                            result = await task
                            if result:
                                processed_count += 1
                        except Exception as e:
                            print(f"å¤„ç†ä¾›åº”å•†æ—¶å‡ºé”™: {e}")
                    
                    # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆå›ºå®š1ç§’å»¶è¿Ÿï¼‰
                    if i + batch_size < len(suppliers):
                        delay = 1.0  # å›ºå®š1ç§’å»¶è¿Ÿ
                        print(f"æ‰¹æ¬¡é—´ç­‰å¾… {delay:.1f} ç§’...")
                        await asyncio.sleep(delay)
            
            print(f"æ‰§ç…§å›¾ç‰‡æå–å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªä¾›åº”å•†")
            return processed_count
            
        except Exception as e:
            print(f"ä»æ•°æ®åº“æå–æ‰§ç…§å›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return 0
    
    async def process_single_supplier(self, company_id, company_name, action_url, proxy, session, log_callback=None):
        """å¤„ç†å•ä¸ªä¾›åº”å•† - ç”¨äºå¹¶å‘å¤„ç†"""
        try:
            self.log(f"å¼€å§‹å¤„ç†: {company_name}", "INFO", log_callback)
            
            # è·å–ä¾›åº”å•†é¡µé¢HTMLï¼ˆå¯ç”¨IPæ£€æµ‹ï¼‰
            html_content = await self.fetch_supplier_page(action_url, proxy, session, check_ip=True, log_callback=log_callback)
            
            if html_content:
                self.log(f"  - {company_name}: æˆåŠŸè·å–é¡µé¢", "SUCCESS", log_callback)
                
                # æå–æ‰§ç…§å›¾ç‰‡
                licenses = await self.extract_licenses_from_html(html_content)
                
                # æå–æ‰§ç…§ä¿¡æ¯
                license_info = self.extract_license_info_from_html(html_content)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # å…ˆåˆ é™¤è¯¥ä¾›åº”å•†çš„æ—§è®°å½•
                cursor.execute('DELETE FROM licenses WHERE supplier_id = ?', (company_id,))
                cursor.execute('DELETE FROM license_info WHERE supplier_id = ?', (company_id,))
                
                # ä¿å­˜æ‰§ç…§å›¾ç‰‡
                if licenses:
                    for license_item in licenses:
                        cursor.execute('''
                            INSERT INTO licenses (supplier_id, license_name, license_url, file_id)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            company_id,
                            license_item['name'],
                            license_item['url'],
                            license_item['fileId']
                        ))
                    print(f"  - {company_name}: æ‰¾åˆ° {len(licenses)} ä¸ªæ‰§ç…§å›¾ç‰‡")
                
                # ä¿å­˜æ‰§ç…§ä¿¡æ¯
                if license_info:
                    cursor.execute('''
                        INSERT INTO license_info (
                            supplier_id, registration_no, company_name, date_of_issue, 
                            date_of_expiry, registered_capital, country_territory, 
                            registered_address, year_established, legal_form, legal_representative
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        company_id,
                        license_info.get('registration_no', ''),
                        license_info.get('company_name', ''),
                        license_info.get('date_of_issue', ''),
                        license_info.get('date_of_expiry', ''),
                        license_info.get('registered_capital', ''),
                        license_info.get('country_territory', ''),
                        license_info.get('registered_address', ''),
                        license_info.get('year_established', ''),
                        license_info.get('legal_form', ''),
                        license_info.get('legal_representative', '')
                    ))
                    print(f"  - {company_name}: æ‰§ç…§ä¿¡æ¯å·²ä¿å­˜")
                
                # å¦‚æœæˆåŠŸæå–åˆ°æ‰§ç…§ä¿¡æ¯ï¼Œæ ‡è®°ä¸ºå·²è·å–å¹¶ä¿å­˜åˆ°æ–‡ä»¶
                if licenses or license_info:
                    cursor.execute('UPDATE suppliers SET license_extracted = TRUE WHERE company_id = ?', (company_id,))
                    
                    # è·å–åˆ†ç±»ä¿¡æ¯
                    cursor.execute('SELECT category_id, category_name FROM suppliers WHERE company_id = ?', (company_id,))
                    category_data = cursor.fetchone()
                    
                    if category_data and category_data[0]:
                        category_id, category_name = category_data
                        # è‡ªåŠ¨ä¿å­˜åˆ°å¯¹åº”åˆ†ç±»ç›®å½•
                        save_path = await self.save_single_supplier_to_category(company_id, company_name, licenses, license_info, category_id, category_name)
                        
                        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä¿å­˜è·¯å¾„
                        if save_path:
                            cursor.execute('UPDATE suppliers SET save_path = ? WHERE company_id = ?', (save_path, company_id))
                    
                    conn.commit()
                
                conn.close()
                
                return bool(licenses or license_info)
            else:
                print(f"  - {company_name}: æ— æ³•è·å–é¡µé¢")
                # æ›´æ–°å¤±è´¥æ¬¡æ•°å’Œæœ€åå°è¯•æ—¶é—´
                self.update_extraction_failure(company_id, company_name)
                return False
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"  - {company_name}: å¤„ç†å‡ºé”™: {e}")
            print(f"  - è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_detail}")
            if log_callback:
                self.log(f"  - {company_name}: å¤„ç†å‡ºé”™: {e}", "ERROR", log_callback)
                self.log(f"  - è¯¦ç»†é”™è¯¯: {error_detail}", "DEBUG", log_callback)
            # æ›´æ–°å¤±è´¥æ¬¡æ•°å’Œæœ€åå°è¯•æ—¶é—´
            self.update_extraction_failure(company_id, company_name)
            return False
    
    def update_extraction_failure(self, company_id, company_name, max_failures=3):
        """æ›´æ–°æå–å¤±è´¥æ¬¡æ•°ï¼Œè¶…è¿‡é˜ˆå€¼è‡ªåŠ¨æ ‡è®°ä¸ºè·³è¿‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ›´æ–°å¤±è´¥æ¬¡æ•°å’Œæœ€åå°è¯•æ—¶é—´
            cursor.execute('''
                UPDATE suppliers 
                SET extraction_failed_count = COALESCE(extraction_failed_count, 0) + 1,
                    last_extraction_attempt = CURRENT_TIMESTAMP
                WHERE company_id = ?
            ''', (company_id,))
            
            # è·å–å½“å‰å¤±è´¥æ¬¡æ•°
            cursor.execute('SELECT extraction_failed_count FROM suppliers WHERE company_id = ?', (company_id,))
            result = cursor.fetchone()
            
            if result and result[0] >= max_failures:
                # è¶…è¿‡å¤±è´¥é˜ˆå€¼ï¼Œè‡ªåŠ¨æ ‡è®°ä¸ºè·³è¿‡
                cursor.execute('UPDATE suppliers SET skip_extraction = TRUE WHERE company_id = ?', (company_id,))
                print(f"  - {company_name}: å¤±è´¥æ¬¡æ•°è¾¾åˆ°{max_failures}æ¬¡ï¼Œå·²è‡ªåŠ¨æ ‡è®°ä¸ºè·³è¿‡æå–")
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"æ›´æ–°å¤±è´¥æ¬¡æ•°æ—¶å‡ºé”™: {e}")
    
    async def extract_single_license(self, company_id, company_name, action_url, proxy=None, log_callback=None):
        """æå–å•ä¸ªä¾›åº”å•†çš„æ‰§ç…§"""
        try:
            self.log(f"å¼€å§‹æå–: {company_name}", "INFO", log_callback)
            
            # è·å–ä¾›åº”å•†é¡µé¢HTML
            html_content = await self.fetch_supplier_page(action_url, proxy, log_callback=log_callback)
            
            if html_content:
                self.log(f"  - {company_name}: æˆåŠŸè·å–é¡µé¢", "SUCCESS", log_callback)
                
                # æå–æ‰§ç…§å›¾ç‰‡
                licenses = await self.extract_licenses_from_html(html_content)
                
                # æå–æ‰§ç…§ä¿¡æ¯
                license_info = self.extract_license_info_from_html(html_content)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # å…ˆåˆ é™¤è¯¥ä¾›åº”å•†çš„æ—§è®°å½•
                cursor.execute('DELETE FROM licenses WHERE supplier_id = ?', (company_id,))
                cursor.execute('DELETE FROM license_info WHERE supplier_id = ?', (company_id,))
                
                # ä¿å­˜æ‰§ç…§å›¾ç‰‡
                if licenses:
                    for license_item in licenses:
                        cursor.execute('''
                            INSERT INTO licenses (supplier_id, license_name, license_url, file_id)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            company_id,
                            license_item['name'],
                            license_item['url'],
                            license_item['fileId']
                        ))
                    print(f"  - {company_name}: æ‰¾åˆ° {len(licenses)} ä¸ªæ‰§ç…§å›¾ç‰‡")
                
                # ä¿å­˜æ‰§ç…§ä¿¡æ¯
                if license_info:
                    cursor.execute('''
                        INSERT INTO license_info (
                            supplier_id, registration_no, company_name, date_of_issue, 
                            date_of_expiry, registered_capital, country_territory, 
                            registered_address, year_established, legal_form, legal_representative
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        company_id,
                        license_info.get('registration_no', ''),
                        license_info.get('company_name', ''),
                        license_info.get('date_of_issue', ''),
                        license_info.get('date_of_expiry', ''),
                        license_info.get('registered_capital', ''),
                        license_info.get('country_territory', ''),
                        license_info.get('registered_address', ''),
                        license_info.get('year_established', ''),
                        license_info.get('legal_form', ''),
                        license_info.get('legal_representative', '')
                    ))
                    print("æ‰§ç…§ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")
                
                # å¦‚æœæˆåŠŸæå–åˆ°æ‰§ç…§ä¿¡æ¯ï¼Œæ ‡è®°ä¸ºå·²è·å–
                if licenses or license_info:
                    cursor.execute('UPDATE suppliers SET license_extracted = TRUE WHERE company_id = ?', (company_id,))
                    print(f"  - {company_name}: æ ‡è®°ä¸ºå·²æå–")
                
                conn.commit()
                conn.close()
                
                return True
            else:
                print(f"  - {company_name}: è·å–é¡µé¢å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"å¤„ç†ä¾›åº”å•†æ—¶å‡ºé”™: {e}")
            return False

    async def recognize_license_from_url(self, action_url, proxy=None, log_callback=None):
        """ä»URLè¯†åˆ«æ‰§ç…§ä¿¡æ¯"""
        try:
            self.log(f"å¼€å§‹è¯†åˆ«æ‰§ç…§: {action_url}", "INFO", log_callback)
            
            # è·å–ä¾›åº”å•†é¡µé¢HTML
            html_content = await self.fetch_supplier_page(action_url, proxy, log_callback=log_callback)
            
            if html_content:
                self.log(f"æˆåŠŸè·å–é¡µé¢", "SUCCESS", log_callback)
                
                # æå–æ‰§ç…§å›¾ç‰‡
                licenses = await self.extract_licenses_from_html(html_content)
                
                # æå–æ‰§ç…§ä¿¡æ¯
                license_info = self.extract_license_info_from_html(html_content)
                
                # è·å–æˆ–åˆ›å»ºä¾›åº”å•†è®°å½•
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # å°è¯•ä»URLä¸­æå–company_id
                company_id = None
                try:
                    # ä»action_urlä¸­æå–company_id
                    match = re.search(r'company_id=(\d+)', action_url)
                    if match:
                        company_id = match.group(1)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œç”Ÿæˆä¸€ä¸ªåŸºäºURLçš„ID
                        company_id = str(hash(action_url) % 1000000000)
                except:
                    company_id = str(hash(action_url) % 1000000000)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥ä¾›åº”å•†
                cursor.execute('SELECT company_id, company_name FROM suppliers WHERE action_url = ?', (action_url,))
                existing_supplier = cursor.fetchone()
                
                if existing_supplier:
                    company_id, company_name = existing_supplier
                else:
                    # åˆ›å»ºæ–°çš„ä¾›åº”å•†è®°å½•
                    company_name = f"ä¾›åº”å•†_{company_id}"
                    cursor.execute('''
                        INSERT INTO suppliers (company_id, company_name, action_url, created_at)
                        VALUES (?, ?, ?, ?)
                    ''', (company_id, company_name, action_url, datetime.now()))
                    conn.commit()
                
                # å…ˆåˆ é™¤è¯¥ä¾›åº”å•†çš„æ—§è®°å½•
                cursor.execute('DELETE FROM licenses WHERE supplier_id = ?', (company_id,))
                cursor.execute('DELETE FROM license_info WHERE supplier_id = ?', (company_id,))
                
                # ä¿å­˜æ‰§ç…§å›¾ç‰‡
                if licenses:
                    for license_item in licenses:
                        cursor.execute('''
                            INSERT INTO licenses (supplier_id, license_name, license_url, file_id)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            company_id,
                            license_item['name'],
                            license_item['url'],
                            license_item['fileId']
                        ))
                    print(f"æ‰¾åˆ° {len(licenses)} ä¸ªæ‰§ç…§å›¾ç‰‡")
                
                # ä¿å­˜æ‰§ç…§ä¿¡æ¯
                if license_info:
                    cursor.execute('''
                        INSERT INTO license_info (
                            supplier_id, registration_no, company_name, date_of_issue, 
                            date_of_expiry, registered_capital, country_territory, 
                            registered_address, year_established, legal_form, legal_representative
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        company_id,
                        license_info.get('registration_no', ''),
                        license_info.get('company_name', ''),
                        license_info.get('date_of_issue', ''),
                        license_info.get('date_of_expiry', ''),
                        license_info.get('registered_capital', ''),
                        license_info.get('country_territory', ''),
                        license_info.get('registered_address', ''),
                        license_info.get('year_established', ''),
                        license_info.get('legal_form', ''),
                        license_info.get('legal_representative', '')
                    ))
                    print("æ‰§ç…§ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")
                
                # æ ‡è®°ä¸ºå·²æå–
                cursor.execute('UPDATE suppliers SET license_extracted = TRUE WHERE company_id = ?', (company_id,))
                
                conn.commit()
                conn.close()
                
                return {
                    'company_id': company_id,
                    'company_name': company_name,
                    'licenses': licenses,
                    'license_info': license_info
                }
            else:
                print("è·å–é¡µé¢å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"è¯†åˆ«æ‰§ç…§æ—¶å‡ºé”™: {e}")
            return None

class AlibabaSupplierCrawlerGUI:
    def __init__(self):
        self.crawler = AlibabaSupplierCrawler()
        self.setup_gui()
    
    def setup_gui(self):
        """è®¾ç½®GUIç•Œé¢"""
        self.root = tk.Tk()
        self.root.title("é˜¿é‡Œå·´å·´ä¾›åº”å•†çˆ¬è™«")
        self.root.geometry("700x500")
        
        # ä¸»æ¡†æ¶
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # è¾“å…¥åŒºåŸŸ
        input_frame = ttk.LabelFrame(main_frame, text="çˆ¬å–è®¾ç½®", padding="10")
        input_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(input_frame, text="æœç´¢å…³é”®è¯:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.keyword_entry = ttk.Entry(input_frame, width=40)
        self.keyword_entry.insert(0, "men's perfume")
        self.keyword_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Label(input_frame, text="é¡µæ•°:").grid(row=1, column=0, sticky=tk.W, pady=5)
        self.pages_entry = ttk.Entry(input_frame, width=40)
        self.pages_entry.insert(0, "1")
        self.pages_entry.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=5)
        
        # ä»£ç†é…ç½®
        ttk.Label(input_frame, text="ä»£ç†æœåŠ¡å™¨:").grid(row=2, column=0, sticky=tk.W, pady=5)
        self.proxy_entry = ttk.Entry(input_frame, width=40)
        # é¢„è®¾æ–°éš§é“ä»£ç†ä¿¡æ¯
        self.proxy_entry.insert(0, "http://t15395136610470:kyhxo4pj@y900.kdltps.com:15818")
        self.proxy_entry.grid(row=2, column=1, sticky=(tk.W, tk.E), pady=5)
        
        # ä»£ç†ä½¿ç”¨å¼€å…³
        self.use_proxy_var = tk.BooleanVar()
        self.use_proxy_check = ttk.Checkbutton(input_frame, text="ä½¿ç”¨ä»£ç†", variable=self.use_proxy_var)
        self.use_proxy_check.grid(row=3, column=1, sticky=tk.W, pady=5)
        
        # æ‰§ç…§å›¾ç‰‡æå–å¼€å…³
        self.extract_licenses_var = tk.BooleanVar()
        self.extract_licenses_check = ttk.Checkbutton(input_frame, text="æå–æ‰§ç…§å›¾ç‰‡", variable=self.extract_licenses_var)
        self.extract_licenses_check.grid(row=4, column=1, sticky=tk.W, pady=5)
        
        # æŒ‰é’®åŒºåŸŸ
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=10)
        
        self.start_button = ttk.Button(button_frame, text="å¼€å§‹çˆ¬å–", command=self.start_crawl)
        self.start_button.pack(side=tk.LEFT, padx=(0, 10))
        
        self.stop_button = ttk.Button(button_frame, text="åœæ­¢", command=self.stop_crawl, state=tk.DISABLED)
        self.stop_button.pack(side=tk.LEFT, padx=(0, 10))
        
        self.extract_button = ttk.Button(button_frame, text="æ‰‹åŠ¨æå–æ‰§ç…§", command=self.manual_extract_licenses)
        self.extract_button.pack(side=tk.LEFT)
        
        # è¿›åº¦åŒºåŸŸ
        progress_frame = ttk.LabelFrame(main_frame, text="è¿›åº¦", padding="10")
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.progress = ttk.Progressbar(progress_frame, orient=tk.HORIZONTAL, mode='determinate')
        self.progress.pack(fill=tk.X, pady=5)
        
        self.status_label = ttk.Label(progress_frame, text="å°±ç»ª")
        self.status_label.pack()
        
        # æ—¥å¿—åŒºåŸŸ
        log_frame = ttk.LabelFrame(main_frame, text="æ—¥å¿—", padding="10")
        log_frame.pack(fill=tk.BOTH, expand=True)
        
        self.log_text = tk.Text(log_frame, height=15, wrap=tk.WORD)
        scrollbar = ttk.Scrollbar(log_frame, orient=tk.VERTICAL, command=self.log_text.yview)
        self.log_text.configure(yscrollcommand=scrollbar.set)
        
        self.log_text.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # é…ç½®ç½‘æ ¼æƒé‡
        input_frame.columnconfigure(1, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(2, weight=1)
        log_frame.columnconfigure(0, weight=1)
        log_frame.rowconfigure(0, weight=1)
    
    def log(self, message):
        """æ·»åŠ æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.log_text.see(tk.END)
        self.root.update()
    
    def parse_proxy(self, proxy_string):
        """è§£æä»£ç†å­—ç¬¦ä¸²"""
        if not proxy_string or not self.use_proxy_var.get():
            return None
        
        try:
            # è§£æä»£ç†æ ¼å¼: http://username:password@host:port
            if '@' in proxy_string:
                # æœ‰è®¤è¯ä¿¡æ¯çš„ä»£ç†
                auth_part, server_part = proxy_string.split('@', 1)
                protocol = auth_part.split('://')[0]
                auth = auth_part.split('://')[1]
                username, password = auth.split(':')
                host, port = server_part.split(':')
                
                return {
                    'host': host,
                    'port': int(port),
                    'username': username,
                    'password': password
                }
            else:
                # æ— è®¤è¯ä¿¡æ¯çš„ä»£ç†
                return None
        except Exception as e:
            print(f"ä»£ç†è§£æå¤±è´¥: {e}")
            return None
    
    def start_crawl(self):
        """å¼€å§‹çˆ¬å–"""
        keyword = self.keyword_entry.get().strip()
        pages = self.pages_entry.get().strip()
        proxy_string = self.proxy_entry.get().strip()
        
        if not keyword:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
            return
        
        try:
            pages = int(pages)
            if pages <= 0:
                messagebox.showerror("é”™è¯¯", "é¡µæ•°å¿…é¡»å¤§äº0ï¼")
                return
        except ValueError:
            messagebox.showerror("é”™è¯¯", "é¡µæ•°å¿…é¡»æ˜¯æ•°å­—ï¼")
            return
        
        # è§£æä»£ç†
        proxy = self.parse_proxy(proxy_string)
        if self.use_proxy_var.get() and not proxy:
            messagebox.showerror("é”™è¯¯", "ä»£ç†æ ¼å¼ä¸æ­£ç¡®ï¼\næ ¼å¼ç¤ºä¾‹: http://username:password@proxy.stock5.com:port")
            return
        
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.progress['value'] = 0
        self.progress['maximum'] = pages
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«
        extract_licenses = self.extract_licenses_var.get()
        self.crawl_thread = threading.Thread(target=self.run_crawler, args=(keyword, pages, proxy, extract_licenses))
        self.crawl_thread.daemon = True
        self.crawl_thread.start()
    
    def run_crawler(self, keyword, pages, proxy, extract_licenses):
        """è¿è¡Œçˆ¬è™«"""
        try:
            if proxy:
                self.log(f"å¼€å§‹çˆ¬å–ä¾›åº”å•†, å…³é”®è¯: {keyword}, é¡µæ•°: {pages}, ä½¿ç”¨ä»£ç†: {proxy['host']}:{proxy['port']}")
            else:
                self.log(f"å¼€å§‹çˆ¬å–ä¾›åº”å•†, å…³é”®è¯: {keyword}, é¡µæ•°: {pages}")
            
            if extract_licenses:
                self.log("å¯ç”¨æ‰§ç…§å›¾ç‰‡æå–åŠŸèƒ½")
            
            # è¿è¡Œå¼‚æ­¥çˆ¬è™«
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            suppliers = loop.run_until_complete(
                self.crawler.crawl_suppliers(keyword, pages, proxy, extract_licenses)
            )
            
            self.log(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(suppliers)} ä¸ªä¾›åº”å•†")
            
            if extract_licenses:
                self.log("æ‰§ç…§å›¾ç‰‡æå–å®Œæˆ")
            
            # æ›´æ–°UI
            self.root.after(0, lambda: self.crawl_finished())
            
        except Exception as e:
            self.log(f"çˆ¬å–å¤±è´¥: {e}")
            self.root.after(0, lambda: self.crawl_finished())
    
    def crawl_finished(self):
        """çˆ¬å–å®Œæˆ"""
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.progress['value'] = 0
        self.status_label.config(text="çˆ¬å–å®Œæˆ")
    
    def stop_crawl(self):
        """åœæ­¢çˆ¬å–"""
        self.log("ç”¨æˆ·åœæ­¢çˆ¬å–")
        self.crawl_finished()
    
    def manual_extract_licenses(self):
        """æ‰‹åŠ¨æå–æ‰§ç…§å›¾ç‰‡"""
        proxy_string = self.proxy_entry.get().strip()
        
        # è§£æä»£ç†
        proxy = self.parse_proxy(proxy_string)
        if self.use_proxy_var.get() and not proxy:
            messagebox.showerror("é”™è¯¯", "ä»£ç†æ ¼å¼ä¸æ­£ç¡®ï¼")
            return
        
        self.log("å¼€å§‹æ‰‹åŠ¨æå–æ‰§ç…§å›¾ç‰‡...")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæå–
        self.extract_thread = threading.Thread(target=self.run_manual_extract, args=(proxy,))
        self.extract_thread.daemon = True
        self.extract_thread.start()
    
    def run_manual_extract(self, proxy):
        """è¿è¡Œæ‰‹åŠ¨æå–"""
        try:
            if proxy:
                self.log(f"ä½¿ç”¨ä»£ç†: {proxy['host']}:{proxy['port']}")
            else:
                self.log("ä¸ä½¿ç”¨ä»£ç†")
            
            # è¿è¡Œå¼‚æ­¥æå–
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            result = loop.run_until_complete(
                self.crawler.extract_licenses_from_database(proxy)
            )
            
            self.log(f"æå–å®Œæˆï¼Œå¤„ç†äº† {result} ä¸ªä¾›åº”å•†")
            
            # æ›´æ–°UI
            self.root.after(0, lambda: self.extract_finished())
            
        except Exception as e:
            self.log(f"æå–å¤±è´¥: {e}")
            self.root.after(0, lambda: self.extract_finished())
    
    def extract_finished(self):
        """æå–å®Œæˆ"""
        self.status_label.config(text="æå–å®Œæˆ")
    
    def run(self):
        """è¿è¡ŒGUI"""
        self.root.mainloop()

if __name__ == "__main__":
    app = AlibabaSupplierCrawlerGUI()
    app.run()